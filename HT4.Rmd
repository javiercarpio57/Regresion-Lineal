---
title: "Modelos de Regresión Lineal"
author: "Javier Carpio & Paul Belches"
date: "19/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Regresión Lineal Multiple

De igual manera que en la hoja de trabajo anterior, las variables a utilizar para la creación de árboles de desición son: 

          * GarageYrBlt
          * GrLivArea
          * X1stFlrSF
          * GarageCars
          * X2ndFlrSF

Así que para mantener una sincronía en el análisis, se utilizarán las mismas variables para poder comparar los resultados.
  
### Prediciendo SalePrice a partir de las demas variables

#### Conjuntos de entrenamiento y prueba.  

Como los datos están balanceados se hizo una partición aleatoria utilizando el 70% de los datos para entrenamiento y el 30% de los datos para prueba. 

```{r, include=FALSE}
library(dplyr)
library(rpart)
library(caret)
library(rpart.plot)
library(randomForest)
library(dplyr) 
library(fpc) 
library(plyr)
library(corrplot)
library(nortest)
library(caret)
```

Filtramos las variables que sean numéricas para analizar su comportamiento.
```{r}
porcentaje<-0.7
datos<-read.csv("train.csv")
datosFilter <-select(datos, GarageYrBlt, GrLivArea, X1stFlrSF, X2ndFlrSF, GarageCars, SalePrice)

#Data cleanup
datosFilter <- na.omit(datosFilter)
```

Se separa el dataset a uno de train y otro de test. Ademas, de las variables antes mencionadas, se quitó X2ndFlrSF pues no tiene una alta correlación.
```{r}
set.seed(123)

corte <- sample(nrow(datosFilter),nrow(datosFilter)*porcentaje)
train<-datos[corte,]
train <-select(datosFilter, GarageYrBlt, GrLivArea, X1stFlrSF, GarageCars, SalePrice)


test<-datos[-corte,]
test <-select(test, GarageYrBlt, GrLivArea, X1stFlrSF, GarageCars, SalePrice)
```

Vamos a hacer un modelo para predecir el precio de la casa con las variables numéricas que son numéricas:   

```{r}
fitMLM_Price<-lm(SalePrice~., data = train)
```

El resumen del modelo:

```{r}
summary(fitMLM_Price)
```

  
Es posible que exista multicolinealidad entre las variables predictoras. Revisemos eso...

```{r}
plot(train)
```

```{r}
matriz_cor <- cor(train)
matriz_cor
corrplot(matriz_cor)
```

  
Tanto como la gráfica y la tabla de correlaciones muestra que cada variable tiene una fuerte relación con SalePrice. Y, además, no existe una alta multicolinealidad entre las variables, así que se pueden utilizar. Ahora, se analizarán los residuos:

```{r}
plot(fitMLM_Price)
```
  

En los gráficos se pueden ver que los residuos no están muy bien distribuidos aleatoriamente a 0, sino que se depesga del eje, aunque, los residuales no siguen ningún patrón. El gráfico Q-Q parece que no sigue una distribución normal, por lo que se verificara:

```{r}
hist(fitMLM_Price$residuals)
boxplot(fitMLM_Price$residuals)
```

La prueba de lilliefors da como resultado que se rechaza la hipotesis de normalidad, es decir, los residuales no se comportan de forma normal.
```{r}
lillie.test(fitMLM_Price$residuals)
```

Es posible que el modelo no sea tan bueno para predecir, pero veremos gráficamente y analíticamente cómo se comporta.
```{r}
predMLM<-predict(fitMLM_Price, newdata = test)
RMSE(predMLM, test$SalePrice, na.rm = TRUE)
```

Vemos que el Error cuadrático medio es de: 39799.01, el cuál es muy alto... Si ploteamos los datos predichos contra los reales:

```{r}
plot(test$SalePrice,col="blue")
points(predMLM, col="red")
```
  
Vemos que el modelo se aproxima a los resultados y predice exactamente a algunos valores reales, sin embargo para valores muy alto o bajo (valores atípico) no los predice correctamente.

La ecuación quedaría de la siguiente forma:

```{r}
fitMLM_Price
```


$SalePrice = `r round(fitMLM_Price$coefficients[1], 2)` + `r round(fitMLM_Price$coefficients[2],2)`*GarageYrBlt + `r round(fitMLM_Price$coefficients[3], 2)`*GrLivArea + `r round(fitMLM_Price$coefficients[4], 2)`* X1stFlrSF + `r round(fitMLM_Price$coefficients[5], 2)`* GarageCars$

Ahora, analicemos, ¿qué pasa con árboles de regresión?

```{r}
houses <- read.csv("train.csv")
housesFilter <-select(houses, LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea,TotRmsAbvGrd,Fireplaces,GarageYrBlt,GarageCars,GarageArea,WoodDeckSF,OpenPorchSF,EnclosedPorch,ScreenPorch,PoolArea,MoSold,SalePrice,YrSold)
```


Se selecciona el 70% del set de prueba, para el entrenamiento del modelo. Y el otro 30% para su testeo. Acontinuación se presentan las dimensiones de cada uno. 

```{r}
porciento <- 70/100
set.seed(123)
trainRowsNumber<-sample(1:nrow(housesFilter),porciento*nrow(housesFilter))
train<-housesFilter[trainRowsNumber,]
test<-housesFilter[-trainRowsNumber,]
```

A continuación se presenta el árbol de regresión generado a partir de el conjunto datos de prueba.

```{r}
#train$grupo <- mapvalues(train$grupo, c(1,2,3), c("Intermedio","Bajo","Caro"))
dt_model<-rpart(train$SalePrice~.,train,method = "anova")
rpart.plot(dt_model)
```

Al comparar, el conjunto de datos de prueba con el conjunto de datos generados por el árbol optenemos. 

```{r}
prediccion <- predict(dt_model, newdata = test)
plot(test$SalePrice, col='blue')
points(prediccion, col='green')
```



